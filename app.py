import streamlit as st
import pandas as pd
import numpy as np
import os
import json
import faiss
from openai import OpenAI
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta
from collections import Counter
from difflib import SequenceMatcher
import hashlib
import logging
import uuid

# dotenv ì•ˆì „ import
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

# -----------------------------------------
# ğŸ”§ 1. Settings & Initialization
# -----------------------------------------
st.set_page_config(page_title="FirstFin - ì‚¬íšŒì´ˆë…„ìƒì„ ìœ„í•œ ë§¥ë½ì¸ì§€í˜• Agent", layout="wide")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATA_PATH = './Data/'
CACHE_PATH = './cache/'
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_VERSION = "v1"

os.makedirs(CACHE_PATH, exist_ok=True)


# -----------------------------------------
# ğŸ”‘ 2. OpenAI Client (Lazy Init)
# -----------------------------------------
@st.cache_resource
def get_openai_client():
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return None
    try:
        return OpenAI(api_key=api_key)
    except Exception as e:
        logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


# -----------------------------------------
# ğŸ’¾ 3. Memory Functions
# -----------------------------------------
def get_memory_path():
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())[:8]
    return f"./FirstFin_memory_{st.session_state.session_id}.txt"


def save_memory(user_msg, assistant_msg):
    try:
        with open(get_memory_path(), "a", encoding="utf-8") as f:
            f.write(f"User: {user_msg}\nAgent: {assistant_msg}\n")
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_memory():
    try:
        path = get_memory_path()
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return f.read()
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return ""


def clear_memory():
    try:
        path = get_memory_path()
        if os.path.exists(path):
            os.remove(path)
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {e}")


# -----------------------------------------
# ğŸ› ï¸ 4. Feature Engineering (TOM)
# -----------------------------------------
TOM_SCHEMA = {
    'required': ['customer_id'],
    'numeric_optional': [
        'SHC_GOLF_GD', 'SHC_VIP_CARD_TF', 'SHC_BUY_LUX_TF',
        'FIN_STOCK_24_4', 'FIN_COIN_24_4',
        'SHC_TRAVEL_AMT_24_4', 'SHC_ENT_AMT_24_4', 'SHC_STARBUCKS_AMT_24_4',
        'SHC_HOTEL_AMT_24_4', 'SHC_M_DF_AMT_24_4', 'SHC_1YEAR_MEAN_AMT',
        'ENT_SVOD_24_4', 'ENT_WEBTOON_24_4', 'COMM_SNS_24_4', 'SHOP_SOCIAL_24_4',
        'SHC_E_COMM_AMT_24_4', 'SHC_DLV_AMT_24_4',
        'SHC_ACCO_AMT_24_4', 'SHC_DEP_AMT_24_4', 'SHC_CLOTHES_AMT_24_4',
        'SHC_FOOD_AMT_24_4', 'SHC_CUL_AMT_24_4', 'NET_ASST_24'
    ],
    'categorical_optional': ['AGE', 'SEX', 'JB_TP']
}


def safe_get_column(df, col, default=0):
    if col in df.columns:
        return pd.to_numeric(df[col], errors='coerce').fillna(default)
    return pd.Series([default] * len(df), index=df.index)


def get_clean_tom_dataset_v2(df):
    temp_df = df.copy()
    if 'customer_id' not in temp_df.columns:
        logger.error("customer_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    for col in TOM_SCHEMA['numeric_optional']:
        temp_df[col] = safe_get_column(temp_df, col, 0)

    temp_df['TOM_Invest'] = (
            safe_get_column(temp_df, 'FIN_STOCK_24_4', 0) +
            safe_get_column(temp_df, 'FIN_COIN_24_4', 0) * 2.0
    )

    yolo_cols = ['SHC_TRAVEL_AMT_24_4', 'SHC_ENT_AMT_24_4', 'SHC_STARBUCKS_AMT_24_4',
                 'SHC_HOTEL_AMT_24_4', 'SHC_M_DF_AMT_24_4']
    total_spend = safe_get_column(temp_df, 'SHC_1YEAR_MEAN_AMT', 1).replace(0, 1)
    yolo_sum = sum(safe_get_column(temp_df, c, 0) for c in yolo_cols)
    temp_df['TOM_YOLO'] = yolo_sum / total_spend

    digital_interest = ['ENT_SVOD_24_4', 'ENT_WEBTOON_24_4', 'COMM_SNS_24_4', 'SHOP_SOCIAL_24_4']
    digital_action = ['SHC_E_COMM_AMT_24_4', 'SHC_DLV_AMT_24_4']
    interest_mean = pd.concat([safe_get_column(temp_df, c, 0) for c in digital_interest], axis=1).mean(axis=1)
    action_sum = sum(safe_get_column(temp_df, c, 0) for c in digital_action)
    temp_df['TOM_Digital'] = interest_mean + (action_sum / total_spend * 5.0)

    categories = {
        'Travel': ['SHC_TRAVEL_AMT_24_4', 'SHC_ACCO_AMT_24_4'],
        'Shopping': ['SHC_DEP_AMT_24_4', 'SHC_CLOTHES_AMT_24_4'],
        'Food': ['SHC_FOOD_AMT_24_4', 'SHC_STARBUCKS_AMT_24_4'],
        'Culture': ['SHC_ENT_AMT_24_4', 'SHC_CUL_AMT_24_4']
    }
    cat_scores = pd.DataFrame(index=temp_df.index)
    for cat, cols in categories.items():
        cat_scores[cat] = sum(safe_get_column(temp_df, c, 0) for c in cols)
    temp_df['TOM_Main_Interest'] = cat_scores.idxmax(axis=1)
    temp_df['TOM_Asset'] = safe_get_column(temp_df, 'NET_ASST_24', 0)

    keep_cols = ['customer_id', 'AGE', 'SEX', 'JB_TP', 'TOM_Invest', 'TOM_YOLO',
                 'TOM_Digital', 'TOM_Asset', 'TOM_Main_Interest']
    return temp_df[[c for c in keep_cols if c in temp_df.columns]].copy()


def create_lifestyle_tom_features(trans_df, profile_df):
    if trans_df.empty or profile_df.empty:
        return profile_df

    local_trans = trans_df.copy()
    local_profile = profile_df.copy()

    try:
        local_trans['transaction_date'] = pd.to_datetime(
            local_trans['transaction_date'], format='mixed', errors='coerce'
        )
        local_trans = local_trans.dropna(subset=['transaction_date'])
    except Exception as e:
        logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return profile_df

    if local_trans.empty:
        return profile_df

    local_trans['day_of_week'] = local_trans['transaction_date'].dt.dayofweek
    local_trans['month_idx'] = (
            local_trans['transaction_date'].dt.year * 12 +
            local_trans['transaction_date'].dt.month
    )

    grouped = local_trans.groupby('customer_id')
    weekend_mask = local_trans['day_of_week'] >= 5
    weekend_spend = local_trans[weekend_mask].groupby('customer_id')['amount'].sum()
    total_spend = grouped['amount'].sum().replace(0, 1)
    tom_weekend = (weekend_spend / total_spend).reindex(local_profile['customer_id']).fillna(0)

    tom_cafe = pd.Series(0.0, index=local_profile['customer_id'])
    tom_conv = pd.Series(0.0, index=local_profile['customer_id'])

    if 'merchant_category' in local_trans.columns:
        cat_counts = local_trans.pivot_table(
            index='customer_id', columns='merchant_category',
            values='transaction_id', aggfunc='count', fill_value=0
        )
        total_counts = grouped['transaction_id'].count().replace(0, 1)
        if 'ì‹ë‹¹/ì¹´í˜' in cat_counts.columns:
            tom_cafe = (cat_counts['ì‹ë‹¹/ì¹´í˜'] / total_counts).reindex(local_profile['customer_id']).fillna(0)
        if 'í¸ì˜ì ' in cat_counts.columns:
            tom_conv = (cat_counts['í¸ì˜ì '] / total_counts).reindex(local_profile['customer_id']).fillna(0)

    slopes = {}
    monthly_spend = local_trans.groupby(['customer_id', 'month_idx'])['amount'].sum().reset_index()
    for cust_id, group in monthly_spend.groupby('customer_id'):
        if len(group) > 1:
            X = group['month_idx'].values.reshape(-1, 1)
            y = group['amount'].values
            mean_y = np.mean(y) if np.mean(y) != 0 else 1
            model = LinearRegression().fit(X, y)
            slopes[cust_id] = model.coef_[0] / mean_y
        else:
            slopes[cust_id] = 0
    tom_trend_raw = pd.Series(slopes, name='TOM_Trend_Raw').reindex(local_profile['customer_id']).fillna(0)

    lifestyle_df = pd.DataFrame({
        'customer_id': local_profile['customer_id'].values,
        'TOM_Weekend': tom_weekend.values,
        'TOM_Cafe': tom_cafe.values,
        'TOM_Conv': tom_conv.values,
        'TOM_Trend_Raw': tom_trend_raw.values
    })
    final_df = pd.merge(local_profile, lifestyle_df, on='customer_id', how='left').fillna(0)

    scaler = MinMaxScaler()
    num_cols = ['AGE', 'TOM_Invest', 'TOM_YOLO', 'TOM_Digital', 'TOM_Asset',
                'TOM_Weekend', 'TOM_Cafe', 'TOM_Conv']
    valid_nums = [c for c in num_cols if c in final_df.columns]
    if valid_nums:
        final_df[valid_nums] = scaler.fit_transform(final_df[valid_nums])

    final_df['TOM_Trend'] = final_df['TOM_Trend_Raw'].clip(-1, 1)

    cat_cols = ['SEX', 'JB_TP', 'TOM_Main_Interest']
    valid_cats = [c for c in cat_cols if c in final_df.columns]
    final_df = pd.get_dummies(final_df, columns=valid_cats, prefix=['Sex', 'Job', 'Interest'])

    return final_df


# -----------------------------------------
# ğŸ“¦ 5. Data Load
# -----------------------------------------
def get_embedding_cache_path(product_db):
    content_hash = hashlib.md5(
        product_db['summary_text'].to_json().encode()
    ).hexdigest()[:8]
    return os.path.join(CACHE_PATH, f"embeddings_{EMBEDDING_MODEL}_{EMBEDDING_VERSION}_{content_hash}.npy")


@st.cache_data(show_spinner="ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
def load_all_data():
    data = {
        'deposit': pd.DataFrame(),
        'card': pd.DataFrame(),
        'customers': pd.DataFrame(),
        'customers_train': pd.DataFrame(),
        'logs': pd.DataFrame(),
        'satisfaction': pd.DataFrame()
    }

    if not os.path.exists(DATA_PATH):
        logger.warning(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_PATH}")
        return data

    try:
        deposit_path = DATA_PATH + "deposit_product_info_ìµœì‹ .xlsx"
        if os.path.exists(deposit_path):
            data['deposit'] = pd.read_excel(deposit_path)
            logger.info(f"ì˜ˆê¸ˆ ìƒí’ˆ ë¡œë“œ: {len(data['deposit'])}ê°œ")
    except Exception as e:
        logger.warning(f"ì˜ˆê¸ˆ ìƒí’ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

    try:
        card_path = DATA_PATH + "card_product_info_ìµœì‹ .xlsx"
        if os.path.exists(card_path):
            data['card'] = pd.read_excel(card_path)
            logger.info(f"ì¹´ë“œ ìƒí’ˆ ë¡œë“œ: {len(data['card'])}ê°œ")
    except Exception as e:
        logger.warning(f"ì¹´ë“œ ìƒí’ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

    try:
        cust_path = DATA_PATH + "customers_with_id.csv"
        if os.path.exists(cust_path):
            raw_cust = pd.read_csv(cust_path)
            data['customers'] = raw_cust
            logger.info(f"ê³ ê° ë°ì´í„° ë¡œë“œ: {len(raw_cust)}ëª…")

            if not raw_cust.empty:
                basic = get_clean_tom_dataset_v2(raw_cust)
                trans_path = DATA_PATH + "card_transactions_updated.csv"
                if os.path.exists(trans_path):
                    raw_trans = pd.read_csv(trans_path)
                    data['customers_train'] = create_lifestyle_tom_features(raw_trans,
                                                                            basic) if not raw_trans.empty else basic
                else:
                    data['customers_train'] = basic
    except Exception as e:
        logger.warning(f"ê³ ê° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    try:
        logs_path = DATA_PATH + "customer_logs.csv"
        if os.path.exists(logs_path):
            data['logs'] = pd.read_csv(logs_path)
            logger.info(f"ë¡œê·¸ ë°ì´í„° ë¡œë“œ: {len(data['logs'])}ê±´")
    except Exception as e:
        logger.warning(f"ë¡œê·¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    try:
        sat_path = DATA_PATH + "customer_satisfaction.csv"
        if os.path.exists(sat_path):
            data['satisfaction'] = pd.read_csv(sat_path)
            logger.info(f"ë§Œì¡±ë„ ë°ì´í„° ë¡œë“œ: {len(data['satisfaction'])}ê±´")
    except Exception as e:
        logger.warning(f"ë§Œì¡±ë„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    return data


def build_product_db(data):
    rows = []
    for _, r in data['deposit'].iterrows():
        rows.append({
            "product_id": r.get('product_id', ''),
            "product_name": r.get('product_name', ''),
            "product_type": "deposit",
            "summary_text": f"[{r.get('product_id', '')}] {r.get('product_name', '')} (ì˜ˆê¸ˆ): ê¸ˆë¦¬ {r.get('max_rate', '')}%"
        })
    for _, r in data['card'].iterrows():
        rows.append({
            "product_id": r.get('product_id', ''),
            "product_name": r.get('product_name', ''),
            "product_type": "card",
            "category": r.get('card_category', ''),
            "summary_text": f"[{r.get('product_id', '')}] {r.get('product_name', '')} (ì¹´ë“œ): í˜œíƒ {str(r.get('benefits', ''))[:100]}"
        })
    return pd.DataFrame(rows)


def build_faiss_index(product_db, client):
    if len(product_db) == 0 or client is None:
        return None

    cache_path = get_embedding_cache_path(product_db)
    embeddings = None

    if os.path.exists(cache_path):
        try:
            embeddings = np.load(cache_path)
            if embeddings.shape[0] != len(product_db) or embeddings.shape[1] != 1536:
                logger.warning("ìºì‹œëœ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜, ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                embeddings = None
                os.remove(cache_path)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            embeddings = None

    if embeddings is None:
        logger.info("ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± ì¤‘...")
        try:
            texts = product_db["summary_text"].tolist()
            batch_size = 50
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                response = client.embeddings.create(
                    model=EMBEDDING_MODEL,
                    input=batch
                )
                batch_embeddings = [d.embedding for d in response.data]
                all_embeddings.extend(batch_embeddings)

            embeddings = np.array(all_embeddings, dtype="float32")
            np.save(cache_path, embeddings)
            logger.info(f"ì„ë² ë”© ìºì‹œ ì €ì¥: {cache_path}")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index


# -----------------------------------------
# ğŸ¤– 6. Recommendation Engines
# -----------------------------------------
def normalize_persona_name(persona_with_suffix):
    base_personas = ['ë°¸ëŸ°ìŠ¤ ë©”ì¸ìŠ¤íŠ¸ë¦¼', 'ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ', 'ì•Œëœ° ì§€í‚´ì´', 'ì‹¤ì† ìŠ¤íƒ€í„°', 'ë””ì§€í„¸ í™ìŠ¤í„°']
    for base in base_personas:
        if base in persona_with_suffix:
            return base
    return 'ì‹¤ì† ìŠ¤íƒ€í„°'


class FirstFinKNNRecommender:
    def __init__(self, df):
        self.model = None
        self.df = None
        self.features = None
        if len(df) == 0:
            return
        self.df = df.set_index('customer_id') if 'customer_id' in df.columns else df
        self.features = self.df.select_dtypes(include=[np.number]).fillna(0)
        if len(self.features.columns) > 0:
            self.model = NearestNeighbors(n_neighbors=min(6, len(self.features)), metric='cosine')
            self.model.fit(self.features)

    def get_similar(self, cid, n=5):
        if self.model is None or self.df is None or cid not in self.df.index:
            return []
        try:
            _, idx = self.model.kneighbors(self.features.loc[[cid]], n_neighbors=min(n + 1, len(self.features)))
            return self.df.iloc[idx[0][1:]].index.tolist()
        except Exception as e:
            logger.warning(f"KNN ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []


class LogRecommender:
    def __init__(self, df, card_df, dep_df):
        self.df = df.copy() if len(df) > 0 else pd.DataFrame()
        if len(self.df) > 0:
            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], errors='coerce')
        self.weights = {'apply': 5, 'compare': 3, 'view': 2, 'click': 1}

    def recommend(self, cid, days=30, k=5):
        if len(self.df) == 0:
            return []
        try:
            cutoff = self.df['timestamp'].max() - timedelta(days=days)
            logs = self.df[(self.df['customer_id'] == cid) & (self.df['timestamp'] >= cutoff)]
            scores = {}
            for _, r in logs.iterrows():
                pid = r['product_id']
                if pid not in scores:
                    scores[pid] = {'score': 0, 'name': r.get('product_name', ''), 'cat': r.get('product_category', '')}
                scores[pid]['score'] += self.weights.get(r.get('action_type', ''), 1)
            return [
                {'product_id': p, 'product_name': d['name'], 'category': d['cat'], 'score': round(d['score'], 2),
                 'source': 'log'}
                for p, d in sorted(scores.items(), key=lambda x: x[1]['score'], reverse=True)[:k]
            ]
        except Exception as e:
            logger.warning(f"ë¡œê·¸ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []

    def summary(self, cid, days=7):
        if len(self.df) == 0:
            return "ë¡œê·¸ ë°ì´í„° ì—†ìŒ"
        try:
            cutoff = self.df['timestamp'].max() - timedelta(days=days)
            logs = self.df[(self.df['customer_id'] == cid) & (self.df['timestamp'] >= cutoff)]
            if len(logs) == 0:
                return "ìµœê·¼ í™œë™ ì—†ìŒ"
            acts = logs['action_type'].value_counts().to_dict()
            prods = logs.groupby('product_name').size().nlargest(3).index.tolist()
            return (f"ìµœê·¼ {days}ì¼ê°„ ì´ {len(logs)}ê±´ í™œë™ ê°ì§€ "
                    f"(í´ë¦­ {acts.get('click', 0)}íšŒ, ì¡°íšŒ {acts.get('view', 0)}íšŒ, "
                    f"ë¹„êµ {acts.get('compare', 0)}íšŒ, ì‹ ì²­ {acts.get('apply', 0)}íšŒ). "
                    f"íŠ¹íˆ '{', '.join(prods)}' ìƒí’ˆì— ë†’ì€ ê´€ì‹¬ì„ ë³´ì„.")
        except Exception as e:
            logger.warning(f"ë¡œê·¸ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"


class SatisfactionRecommender:
    def __init__(self, df, card_df, dep_df):
        self.df = df
        self.card_df = card_df
        self.dep_df = dep_df

    def recommend(self, cid, similar_ids, k=5):
        if len(self.df) == 0 or not similar_ids:
            return []
        try:
            sim = self.df[self.df['customer_id'].isin(similar_ids)]
            stats = sim[sim['rating'] >= 4.0].groupby(
                ['product_id', 'product_name', 'product_type']
            ).agg({'rating': 'mean', 'customer_id': 'count'}).reset_index()
            stats['score'] = stats['rating'] * np.log1p(stats['customer_id'])
            return [
                {'product_id': r['product_id'], 'product_name': r['product_name'], 'score': round(r['score'], 2),
                 'source': 'satisfaction'}
                for _, r in stats.nlargest(k, 'score').iterrows()
            ]
        except Exception as e:
            logger.warning(f"ë§Œì¡±ë„ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []


class ZeroShotRecommender:
    PROFILES = {
        0: {'name': 'ë°¸ëŸ°ìŠ¤ ë©”ì¸ìŠ¤íŠ¸ë¦¼', 'keywords': ['ì¼ìƒ', 'ìƒí™œ', 'ì§ì¥ì¸'], 'card_keywords': ['ì¼ìƒ', 'ìƒí™œ', 'ì§ì¥ì¸'],
            'dep_keywords': ['ì˜ˆê¸ˆ', 'ì…ì¶œê¸ˆ', 'ììœ ']},
        1: {'name': 'ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ', 'keywords': ['ì—¬í–‰', 'ì‡¼í•‘', 'í”„ë¦¬ë¯¸ì—„'], 'card_keywords': ['ì—¬í–‰', 'ì‡¼í•‘', 'í”„ë¦¬ë¯¸ì—„', 'í•­ê³µ'],
            'dep_keywords': ['ì˜ˆê¸ˆ', 'íˆ¬ì', 'ê³ ê¸ˆë¦¬']},
        2: {'name': 'ì•Œëœ° ì§€í‚´ì´', 'keywords': ['ìƒí™œ', 'ë§ˆíŠ¸', 'ê³µê³¼ê¸ˆ'], 'card_keywords': ['ìƒí™œ', 'ë§ˆíŠ¸', 'í• ì¸', 'ìºì‹œë°±'],
            'dep_keywords': ['ì ê¸ˆ', 'ì˜ˆê¸ˆ', 'ì•ˆì „']},
        3: {'name': 'ì‹¤ì† ìŠ¤íƒ€í„°', 'keywords': ['ì²­ë…„', 'êµí†µ', 'í†µì‹ '], 'card_keywords': ['ì²­ë…„', 'êµí†µ', 'í†µì‹ ', 'í•™ìƒ'],
            'dep_keywords': ['ì ê¸ˆ', 'ì²­ë…„', 'ëª©ëˆ']},
        4: {'name': 'ë””ì§€í„¸ í™ìŠ¤í„°', 'keywords': ['ì‡¼í•‘', 'ë””ì§€í„¸', 'ë¬¸í™”'], 'card_keywords': ['ì‡¼í•‘', 'ë””ì§€í„¸', 'ì˜¨ë¼ì¸', 'êµ¬ë…'],
            'dep_keywords': ['ì…ì¶œê¸ˆ', 'ì ê¸ˆ', 'ëª¨ë°”ì¼']}
    }

    def __init__(self, cust_df, card_df, dep_df, log_df, sat_df):
        self.cust_df = cust_df
        self.card_df = card_df
        self.dep_df = dep_df
        self.log_df = log_df
        self.sat_df = sat_df

    def is_cold(self, cid):
        if len(self.log_df) == 0:
            return True
        return len(self.log_df[self.log_df['customer_id'] == cid]) < 5

    def _fuzzy_match(self, text, keywords, threshold=0.4):
        if pd.isna(text):
            return 0
        text = str(text).lower()
        max_score = 0
        for kw in keywords:
            if kw.lower() in text:
                max_score = max(max_score, 1.0)
            else:
                ratio = SequenceMatcher(None, kw.lower(), text).ratio()
                max_score = max(max_score, ratio)
        return max_score if max_score >= threshold else 0

    def recommend(self, cid, k=5):
        if len(self.cust_df) == 0:
            return []
        cust_row = self.cust_df[self.cust_df['customer_id'] == cid]
        persona_id = 3 if cust_row.empty else int(cust_row.iloc[0].get('Persona_Cluster', 3))
        profile = self.PROFILES.get(persona_id, self.PROFILES[3])
        results = []

        if len(self.card_df) > 0 and 'card_category' in self.card_df.columns:
            card_scores = self.card_df.copy()
            card_scores['match_score'] = card_scores['card_category'].apply(
                lambda x: self._fuzzy_match(x, profile['card_keywords']))
            if 'product_name' in card_scores.columns:
                card_scores['name_score'] = card_scores['product_name'].apply(
                    lambda x: self._fuzzy_match(x, profile['keywords']))
                card_scores['total_score'] = card_scores['match_score'] * 0.7 + card_scores['name_score'] * 0.3
            else:
                card_scores['total_score'] = card_scores['match_score']
            for _, card in card_scores[card_scores['total_score'] > 0].nlargest(3, 'total_score').iterrows():
                results.append({'product_id': card['product_id'], 'product_name': card['product_name'],
                                'score': round(card['total_score'] * 5, 2), 'reason': f"{profile['name']} ë§ì¶¤ ì¶”ì²œ",
                                'source': 'zeroshot'})

        if len(self.dep_df) > 0 and 'product_name' in self.dep_df.columns:
            dep_scores = self.dep_df.copy()
            dep_scores['match_score'] = dep_scores['product_name'].apply(
                lambda x: self._fuzzy_match(x, profile['dep_keywords']))
            for _, dep in dep_scores[dep_scores['match_score'] > 0].nlargest(2, 'match_score').iterrows():
                results.append({'product_id': dep['product_id'], 'product_name': dep['product_name'],
                                'score': round(dep['match_score'] * 4, 2), 'reason': f"{profile['name']} ë§ì¶¤ ì¶”ì²œ",
                                'source': 'zeroshot'})

        return results[:k]

    def recommend_by_persona_name(self, persona_name, k=5):
        """í˜ë¥´ì†Œë‚˜ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¶”ì²œ (ë¹„íšŒì›ìš©)"""
        normalized = normalize_persona_name(persona_name)
        profile = None
        for pid, p in self.PROFILES.items():
            if p['name'] == normalized:
                profile = p
                break
        if profile is None:
            profile = self.PROFILES[3]  # ê¸°ë³¸ê°’: ì‹¤ì† ìŠ¤íƒ€í„°

        results = []

        if len(self.card_df) > 0 and 'card_category' in self.card_df.columns:
            card_scores = self.card_df.copy()
            card_scores['match_score'] = card_scores['card_category'].apply(
                lambda x: self._fuzzy_match(x, profile['card_keywords']))
            if 'product_name' in card_scores.columns:
                card_scores['name_score'] = card_scores['product_name'].apply(
                    lambda x: self._fuzzy_match(x, profile['keywords']))
                card_scores['total_score'] = card_scores['match_score'] * 0.7 + card_scores['name_score'] * 0.3
            else:
                card_scores['total_score'] = card_scores['match_score']
            for _, card in card_scores[card_scores['total_score'] > 0].nlargest(3, 'total_score').iterrows():
                results.append({'product_id': card['product_id'], 'product_name': card['product_name'],
                                'score': round(card['total_score'] * 5, 2), 'reason': f"{profile['name']} ë§ì¶¤ ì¶”ì²œ",
                                'source': 'zeroshot'})

        if len(self.dep_df) > 0 and 'product_name' in self.dep_df.columns:
            dep_scores = self.dep_df.copy()
            dep_scores['match_score'] = dep_scores['product_name'].apply(
                lambda x: self._fuzzy_match(x, profile['dep_keywords']))
            for _, dep in dep_scores[dep_scores['match_score'] > 0].nlargest(2, 'match_score').iterrows():
                results.append({'product_id': dep['product_id'], 'product_name': dep['product_name'],
                                'score': round(dep['match_score'] * 4, 2), 'reason': f"{profile['name']} ë§ì¶¤ ì¶”ì²œ",
                                'source': 'zeroshot'})

        return results[:k]


def run_rule_engine(profile_name, intent, card_df, dep_df):
    normalized_profile = normalize_persona_name(profile_name)
    RULES = {
        'ì‹¤ì† ìŠ¤íƒ€í„°': {'default': {'card': ['ì²­ë…„', 'êµí†µ', 'í†µì‹ '], 'deposit': ['ì ê¸ˆ', 'ì²­ë…„']},
                   'ì—¬í–‰': {'card': ['í•­ê³µ', 'ì—¬í–‰'], 'deposit': ['ì—¬í–‰', 'ì ê¸ˆ']},
                   'ì €ì¶•': {'card': ['ìºì‹œë°±'], 'deposit': ['ì ê¸ˆ', 'ì •ê¸°']}},
        'ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ': {'default': {'card': ['í”„ë¦¬ë¯¸ì—„', 'ì—¬í–‰', 'ì‡¼í•‘'], 'deposit': ['ê³ ê¸ˆë¦¬', 'ì˜ˆê¸ˆ']},
                    'ì—¬í–‰': {'card': ['í•­ê³µ', 'VIP'], 'deposit': ['ì™¸í™”']}, 'ì‡¼í•‘': {'card': ['ì‡¼í•‘', 'ë°±í™”ì '], 'deposit': ['ììœ ']}},
        'ì•Œëœ° ì§€í‚´ì´': {'default': {'card': ['ìºì‹œë°±', 'ë§ˆíŠ¸', 'ìƒí™œ'], 'deposit': ['ì ê¸ˆ', 'ì˜ˆê¸ˆ']},
                   'ì €ì¶•': {'card': ['ì ë¦½'], 'deposit': ['ì •ê¸°ì ê¸ˆ']}},
        'ë””ì§€í„¸ í™ìŠ¤í„°': {'default': {'card': ['ì˜¨ë¼ì¸', 'ì‡¼í•‘', 'êµ¬ë…'], 'deposit': ['ëª¨ë°”ì¼', 'ì…ì¶œê¸ˆ']},
                    'êµ¬ë…': {'card': ['ìŠ¤íŠ¸ë¦¬ë°'], 'deposit': ['ììœ ']}},
        'ë°¸ëŸ°ìŠ¤ ë©”ì¸ìŠ¤íŠ¸ë¦¼': {'default': {'card': ['ì¼ìƒ', 'ìƒí™œ'], 'deposit': ['ì˜ˆê¸ˆ', 'ììœ ']}}
    }
    intent_map = {'ì—¬í–‰': ['ì—¬í–‰', 'í•´ì™¸', 'í•­ê³µ'], 'ì €ì¶•': ['ì €ì¶•', 'ì ê¸ˆ', 'ëª©ëˆ'], 'ì‡¼í•‘': ['ì‡¼í•‘', 'ë°±í™”ì '], 'êµ¬ë…': ['êµ¬ë…', 'ë„·í”Œë¦­ìŠ¤']}
    detected_intent = 'default'
    intent_lower = intent.lower() if intent else ''
    for key, keywords in intent_map.items():
        if any(kw in intent_lower for kw in keywords):
            detected_intent = key
            break
    profile_rules = RULES.get(normalized_profile, RULES['ì‹¤ì† ìŠ¤íƒ€í„°'])
    rule = profile_rules.get(detected_intent, profile_rules['default'])
    results = []
    if len(card_df) > 0 and 'card_category' in card_df.columns:
        for kw in rule['card']:
            matches = card_df[card_df['card_category'].str.contains(kw, case=False, na=False)]
            for _, r in matches.head(1).iterrows():
                results.append(
                    {'product_id': r['product_id'], 'product_name': r['product_name'], 'reason': f"'{kw}' í‚¤ì›Œë“œ ë§¤ì¹­",
                     'source': 'rule'})
    if len(dep_df) > 0 and 'product_name' in dep_df.columns:
        for kw in rule['deposit']:
            matches = dep_df[dep_df['product_name'].str.contains(kw, case=False, na=False)]
            for _, r in matches.head(1).iterrows():
                results.append(
                    {'product_id': r['product_id'], 'product_name': r['product_name'], 'reason': f"'{kw}' í‚¤ì›Œë“œ ë§¤ì¹­",
                     'source': 'rule'})
    seen = set()
    return [r for r in results if not (r['product_id'] in seen or seen.add(r['product_id']))][:5]


class HybridEngine:
    WEIGHTS = {'satisfaction': 0.4, 'knn': 0.35, 'log': 0.25}

    def __init__(self, data):
        self.knn = FirstFinKNNRecommender(data.get('customers_train', pd.DataFrame()))
        self.zero = ZeroShotRecommender(data['customers'], data['card'], data['deposit'], data['logs'],
                                        data['satisfaction'])
        self.log = LogRecommender(data['logs'], data['card'], data['deposit'])
        self.sat = SatisfactionRecommender(data['satisfaction'], data['card'], data['deposit'])
        self.raw_customers = data.get('customers', pd.DataFrame())
        self.tom_df = data.get('customers_train', pd.DataFrame())
        self.card_df = data['card']
        self.dep_df = data['deposit']

    def get_tom_profile(self, cid):
        if self.tom_df.empty:
            return {"status": "ë°ì´í„° ì—†ìŒ"}
        try:
            if 'customer_id' not in self.tom_df.columns:
                return {"status": "ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜"}
            tom_indexed = self.tom_df.set_index('customer_id')
            if cid not in tom_indexed.index:
                return {"status": "ê³ ê° ID ì—†ìŒ"}
            row = tom_indexed.loc[cid]
            trend_raw = row.get('TOM_Trend_Raw', row.get('TOM_Trend', 0))
            return {"Trend": f"{trend_raw:.1%}", "YOLO": f"{row.get('TOM_YOLO', 0):.2f}",
                    "Digital": f"{row.get('TOM_Digital', 0):.2f}", "Weekend": f"{row.get('TOM_Weekend', 0):.2f}"}
        except Exception as e:
            logger.warning(f"TOM í”„ë¡œí•„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "ì¡°íšŒ ì‹¤íŒ¨"}

    def get_persona_name(self, cid):
        if self.raw_customers.empty:
            return "ì‹¤ì† ìŠ¤íƒ€í„°"
        row = self.raw_customers[self.raw_customers['customer_id'] == cid]
        if row.empty:
            return "ì‹¤ì† ìŠ¤íƒ€í„°"
        base = self.zero.PROFILES.get(int(row.iloc[0].get('Persona_Cluster', 3)), {}).get('name', 'ì‹¤ì† ìŠ¤íƒ€í„°')
        if not self.tom_df.empty and 'customer_id' in self.tom_df.columns:
            tom_indexed = self.tom_df.set_index('customer_id')
            if cid in tom_indexed.index:
                t = tom_indexed.loc[cid]
                if t.get('TOM_YOLO', 0) > 0.7:
                    return f"ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ (ìµœê·¼ ì†Œë¹„ ê¸‰ì¦)"
                trend_raw = t.get('TOM_Trend_Raw', t.get('TOM_Trend', 0))
                if trend_raw < -0.1:
                    return f"ì•Œëœ° ì§€í‚´ì´ (ì ˆì•½ ëª¨ë“œ)"
        return base

    def recommend(self, cid, k=3):
        """ê¸°ì¡´ íšŒì›ìš© í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ"""
        if self.zero.is_cold(cid):
            return {'recs': self.zero.recommend(cid, k), 'is_cold': True, 'ctx': {'log_sum': "ì‹ ê·œ ê³ ê° - ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ"}}
        similar = self.knn.get_similar(cid)
        log_recs = self.log.recommend(cid, k=10)
        sat_recs = self.sat.recommend(cid, similar, k=10)
        merged = {}
        for r in log_recs:
            merged[r['product_id']] = {'info': r, 'score': r['score'] * self.WEIGHTS['log']}
        for r in sat_recs:
            if r['product_id'] in merged:
                merged[r['product_id']]['score'] += r['score'] * self.WEIGHTS['satisfaction']
            else:
                merged[r['product_id']] = {'info': r, 'score': r['score'] * self.WEIGHTS['satisfaction']}
        final = sorted(merged.values(), key=lambda x: x['score'], reverse=True)[:k]
        return {'recs': [f['info'] for f in final], 'is_cold': False, 'ctx': {'log_sum': self.log.summary(cid)}}

    def recommend_guest(self, persona_name, k=3):
        """ë¹„íšŒì›ìš© í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ"""
        recs = self.zero.recommend_by_persona_name(persona_name, k)
        return {
            'recs': recs,
            'is_cold': True,
            'ctx': {'log_sum': f"ë¹„íšŒì› - '{normalize_persona_name(persona_name)}' í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ"}
        }


# -----------------------------------------
# ğŸ¨ 7. Streamlit UI
# -----------------------------------------
st.title("ğŸ¤– FirstFin - ì‚¬íšŒì´ˆë…„ìƒì„ ìœ„í•œ ì€í–‰ ìƒí’ˆ ì¶”ì²œ Agent")
st.markdown("**:blue[TOM(Time-Occasion-Method)]** ë° **:green[Lifestyle]** ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ")

# ë°ì´í„° ë¡œë“œ
data = load_all_data()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = get_openai_client()

# ì œí’ˆ DB ìƒì„±
product_db = build_product_db(data)

# FAISS ì¸ë±ìŠ¤ ìƒì„±
if 'faiss_index' not in st.session_state:
    st.session_state.faiss_index = None
    if client is not None and len(product_db) > 0:
        with st.spinner("ğŸ”„ ì„ë² ë”© ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ìµœì´ˆ 1íšŒ)"):
            st.session_state.faiss_index = build_faiss_index(product_db, client)

index = st.session_state.faiss_index

# ì—”ì§„ ì´ˆê¸°í™”
engine = HybridEngine(data)


# -----------------------------------------
# ğŸ”§ 8. Tool Functions
# -----------------------------------------
def validate_tool_args(fn_name, args):
    validators = {
        'run_hybrid': lambda a: 'cid' in a and isinstance(a.get('cid'), str),
        'run_rule': lambda a: 'profile' in a and 'intent' in a,
        'get_details': lambda a: 'pids' in a and isinstance(a.get('pids'), (list, str)),
        'search_info': lambda a: 'query' in a and isinstance(a.get('query'), str)
    }
    validator = validators.get(fn_name)
    if validator is None:
        return False, f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {fn_name}"
    if not validator(args):
        return False, f"ì˜ëª»ëœ ì¸ì: {fn_name}({args})"
    return True, None


def run_hybrid(cid, intent=""):
    """ê¸°ì¡´ íšŒì›ìš© ì¶”ì²œ"""
    try:
        r = engine.recommend(cid, 3)
        return json.dumps({"recommendations": r['recs'], "context": r['ctx'], "is_cold_start": r['is_cold']},
                          ensure_ascii=False)
    except Exception as e:
        logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


def run_rule(profile, intent):
    """ë¹„íšŒì›ìš© ë£° ê¸°ë°˜ ì¶”ì²œ"""
    try:
        results = run_rule_engine(profile, intent, data['card'], data['deposit'])
        return json.dumps({"recommendations": results, "profile": profile, "detected_intent": intent},
                          ensure_ascii=False)
    except Exception as e:
        logger.error(f"ë£° ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


def get_details(pids):
    try:
        if isinstance(pids, str):
            pids = json.loads(pids)
        details = []
        for p in pids:
            matches = product_db[product_db['product_id'] == p]
            if len(matches) > 0:
                details.append(matches.iloc[0]['summary_text'])
            else:
                details.append(f"[{p}] ìƒí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return "\n".join(details)
    except Exception as e:
        logger.error(f"ìƒí’ˆ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return f"ì¡°íšŒ ì‹¤íŒ¨: {e}"


def search_info(query):
    if index is None or client is None:
        return "ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    try:
        response = client.embeddings.create(model=EMBEDDING_MODEL, input=query)
        q_vec = np.array([response.data[0].embedding], dtype="float32")
        _, I = index.search(q_vec, 3)
        results = [product_db.iloc[i]["summary_text"] for i in I[0] if i < len(product_db)]
        return "\n".join(results) if results else "ê´€ë ¨ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}"


def run_agent(user_input, user_mode, cid=None, persona=None):
    """
    ì—ì´ì „íŠ¸ ì‹¤í–‰
    - user_mode: 'member' (ê¸°ì¡´ íšŒì›) ë˜ëŠ” 'guest' (ë¹„íšŒì›)
    - cid: íšŒì›ì¼ ê²½ìš° ê³ ê° ID
    - persona: ë¹„íšŒì›ì¼ ê²½ìš° ì„ íƒí•œ í˜ë¥´ì†Œë‚˜
    """
    if client is None:
        return "âš ï¸ OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

    # ëª¨ë“œì— ë”°ë¥¸ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
    if user_mode == 'member' and cid:
        analyzed_persona = engine.get_persona_name(cid)
        tom_info = engine.get_tom_profile(cid)
        log_summary = engine.log.summary(cid)
        tom_insight = f"(TOMì§€í‘œ: {json.dumps(tom_info, ensure_ascii=False)})\n[ìµœê·¼ í–‰ë™ ë¡œê·¸]: {log_summary}"
        context_type = "ê¸°ì¡´ íšŒì›"
        tool_instruction = "ë°˜ë“œì‹œ `run_hybrid` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”."
    else:
        analyzed_persona = persona or "ì‹¤ì† ìŠ¤íƒ€í„°"
        tom_insight = "(ë¹„íšŒì› - í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ)"
        context_type = "ë¹„íšŒì›/ì‹ ê·œ"
        tool_instruction = "ë°˜ë“œì‹œ `run_rule` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”."

    sys_msg = f"""
# Role: ê¸ˆìœµ AI íŒŒíŠ¸ë„ˆ 'FirstFin'

## Context
- ì‚¬ìš©ì ìœ í˜•: {context_type}
- ê³ ê° ID: {cid or 'Guest'}
- í˜ë¥´ì†Œë‚˜: "{analyzed_persona}"
- ë°ì´í„° ë¶„ì„: {tom_insight}

## Guidelines
1. {tool_instruction}
2. ì¶”ì²œ ìƒí’ˆì€ ë°˜ë“œì‹œ `get_details`ë¡œ í˜œíƒ í™•ì¸ í›„ ì„¤ëª…
3. í—ˆìœ„ í˜œíƒ ì ˆëŒ€ ê¸ˆì§€, Tool ê²°ê³¼ë§Œ ì‚¬ìš©
4. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤, ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš©
5. ì‚¬íšŒì´ˆë…„ìƒ ëˆˆë†’ì´ì— ë§ì¶° ì‰½ê²Œ ì„¤ëª…
"""

    msgs = [{"role": "system", "content": sys_msg}]
    mem = load_memory()
    if mem:
        msgs.append({"role": "user", "content": f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n{mem[-600:]}"})
    msgs.append({"role": "user", "content": user_input})

    tools = [
        {"type": "function",
         "function": {"name": "run_hybrid", "description": "ê¸°ì¡´ íšŒì›ìš©: ê³ ê° ID ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ (ë¡œê·¸, ë§Œì¡±ë„, ìœ ì‚¬ê³ ê° ë¶„ì„)",
                      "parameters": {"type": "object", "properties": {"cid": {"type": "string", "description": "ê³ ê° ID"},
                                                                      "intent": {"type": "string",
                                                                                 "description": "ì‚¬ìš©ì ì˜ë„"}},
                                     "required": ["cid"]}}},
        {"type": "function", "function": {"name": "run_rule", "description": "ë¹„íšŒì›ìš©: í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë£° ì¶”ì²œ",
                                          "parameters": {"type": "object", "properties": {"profile": {"type": "string",
                                                                                                      "description": "í˜ë¥´ì†Œë‚˜ ì´ë¦„ (ì‹¤ì† ìŠ¤íƒ€í„°, ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ ë“±)"},
                                                                                          "intent": {"type": "string",
                                                                                                     "description": "ì‚¬ìš©ì ì˜ë„"}},
                                                         "required": ["profile", "intent"]}}},
        {"type": "function", "function": {"name": "get_details", "description": "ìƒí’ˆ IDë¡œ ìƒì„¸ í˜œíƒ ì¡°íšŒ",
                                          "parameters": {"type": "object", "properties": {
                                              "pids": {"type": "array", "items": {"type": "string"}}},
                                                         "required": ["pids"]}}},
        {"type": "function", "function": {"name": "search_info", "description": "í‚¤ì›Œë“œë¡œ ìƒí’ˆ ê²€ìƒ‰",
                                          "parameters": {"type": "object", "properties": {"query": {"type": "string"}},
                                                         "required": ["query"]}}}
    ]

    try:
        res = client.chat.completions.create(model="gpt-4o", messages=msgs, tools=tools, tool_choice="auto",
                                             temperature=0.1)
        msg = res.choices[0].message
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return f"âš ï¸ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    if msg.tool_calls:
        msgs.append(msg)
        for tc in msg.tool_calls:
            fn = tc.function.name
            try:
                args = json.loads(tc.function.arguments)
            except json.JSONDecodeError as e:
                msgs.append({"role": "tool", "tool_call_id": tc.id, "name": fn, "content": f"ì¸ì íŒŒì‹± ì˜¤ë¥˜: {e}"})
                continue

            is_valid, error_msg = validate_tool_args(fn, args)
            if not is_valid:
                msgs.append({"role": "tool", "tool_call_id": tc.id, "name": fn, "content": f"ê²€ì¦ ì‹¤íŒ¨: {error_msg}"})
                continue

            if fn == "run_hybrid":
                # ê¸°ì¡´ íšŒì›ìš©
                result = run_hybrid(args.get('cid') or cid, args.get('intent', ''))
            elif fn == "run_rule":
                # ë¹„íšŒì›ìš©
                result = run_rule(args.get('profile') or analyzed_persona, args.get('intent', ''))
            elif fn == "get_details":
                result = get_details(args.get('pids', []))
            elif fn == "search_info":
                result = search_info(args.get('query', ''))
            else:
                result = "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ì…ë‹ˆë‹¤."
            msgs.append({"role": "tool", "tool_call_id": tc.id, "name": fn, "content": str(result)})

        try:
            final = client.chat.completions.create(model="gpt-4o", messages=msgs, temperature=0.7)
            answer = final.choices[0].message.content
        except Exception as e:
            logger.error(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            answer = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    else:
        answer = msg.content

    save_memory(user_input, answer)
    return answer


# -----------------------------------------
# ğŸ›ï¸ 9. Sidebar - ëª…í™•í•œ ëª¨ë“œ ë¶„ë¦¬
# -----------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‚¬ìš©ì ì„¤ì •")

    if client is None:
        st.error("âš ï¸ API Key ë¯¸ì„¤ì •")
        st.caption("`.env` íŒŒì¼ì— `OPENAI_API_KEY=sk-...` ì¶”ê°€")
    else:
        st.success("âœ… API ì—°ê²°ë¨")

    st.divider()

    # âœ… ëª¨ë“œ ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼ìœ¼ë¡œ ëª…í™•íˆ ë¶„ë¦¬)
    user_mode = st.radio(
        "ğŸ” ì‚¬ìš©ì ìœ í˜• ì„ íƒ",
        options=["guest", "member"],
        format_func=lambda x: "ğŸ‘¤ ë¹„íšŒì› / ì‹ ê·œ ë°©ë¬¸ì" if x == "guest" else "ğŸ¦ ê¸°ì¡´ íšŒì› (ID ë¡œê·¸ì¸)",
        index=0,
        help="ê¸°ì¡´ íšŒì›ì€ ì¶•ì ëœ ë°ì´í„° ê¸°ë°˜ ì´ˆê°œì¸í™” ì¶”ì²œ, ë¹„íšŒì›ì€ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ"
    )

    st.divider()

    # ëª¨ë“œì— ë”°ë¥¸ UI ë¶„ê¸°
    cid_input = None
    selected_persona = None

    if user_mode == "member":
        # ğŸ¦ ê¸°ì¡´ íšŒì› ëª¨ë“œ
        st.subheader("ğŸ¦ ê¸°ì¡´ íšŒì› ë¡œê·¸ì¸")
        cid_input = st.text_input(
            "ê³ ê° ID ì…ë ¥",
            placeholder="ì˜ˆ: C00001",
            help="ê³ ê° IDë¥¼ ì…ë ¥í•˜ë©´ ê±°ë˜ ë‚´ì—­, ê´€ì‹¬ ìƒí’ˆ ë“±ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤."
        )

        if cid_input:
            # ID ìœ íš¨ì„± ê²€ì¦
            if cid_input in data['customers']['customer_id'].values:
                st.success(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {cid_input}")

                # ê³ ê° ì •ë³´ ë¯¸ë¦¬ë³´ê¸°
                persona_name = engine.get_persona_name(cid_input)
                st.info(f"ğŸ¯ ë¶„ì„ëœ í˜ë¥´ì†Œë‚˜: **{persona_name}**")
            else:
                st.warning(f"âš ï¸ '{cid_input}' IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„íšŒì› ëª¨ë“œë¡œ ì „í™˜í•˜ì„¸ìš”.")
                cid_input = None  # ìœ íš¨í•˜ì§€ ì•Šì€ IDëŠ” ë¬´ì‹œ
        else:
            st.caption("ğŸ’¡ ê³ ê° IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    else:
        # ğŸ‘¤ ë¹„íšŒì› ëª¨ë“œ
        st.subheader("ğŸ‘¤ ë¹„íšŒì› / ì‹ ê·œ ë°©ë¬¸ì")
        st.caption("ë³¸ì¸ì˜ ì†Œë¹„ ì„±í–¥ê³¼ ê°€ì¥ ê°€ê¹Œìš´ í˜ë¥´ì†Œë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

        persona_map = {
            "ì‹¤ì† ìŠ¤íƒ€í„°": "ğŸ“ ì‚¬íšŒì´ˆë…„ìƒ | ëª©ëˆ ë§ˆë ¨, êµí†µ/í†µì‹ ë¹„ í• ì¸ ì¤‘ì‹œ",
            "ìŠ¤ë§ˆíŠ¸ í”Œë ‰ì„œ": "âœˆï¸ YOLO | ì—¬í–‰, í˜¸ìº‰ìŠ¤, ëª…í’ˆ ì†Œë¹„ ì„ í˜¸",
            "ë””ì§€í„¸ í™ìŠ¤í„°": "ğŸ“± íŠ¸ë Œë“œ | ë„·í”Œë¦­ìŠ¤, ê°„í¸ê²°ì œ í˜œíƒ í•„ìˆ˜",
            "ì•Œëœ° ì§€í‚´ì´": "ğŸ’° ì ˆì•½ | ë§ˆíŠ¸/ê³µê³¼ê¸ˆ í• ì¸ ìµœìš°ì„ ",
            "ë°¸ëŸ°ìŠ¤ ë©”ì¸ìŠ¤íŠ¸ë¦¼": "â˜• ì§ì¥ì¸ | ì ì‹¬/ì»¤í”¼ ë“± ë¬´ë‚œí•œ í˜œíƒ"
        }

        selected_persona = st.selectbox(
            "ë‚˜ì˜ ì†Œë¹„ ì„±í–¥ì€?",
            options=list(persona_map.keys()),
            index=0
        )
        st.info(f"ğŸ’¡ {persona_map[selected_persona]}")

    st.divider()

    # ì‹œìŠ¤í…œ ìƒíƒœ
    with st.expander("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ"):
        st.write(f"â€¢ ê³ ê°: {len(data['customers']):,}ëª…")
        st.write(f"â€¢ ë¡œê·¸: {len(data['logs']):,}ê±´")
        st.write(f"â€¢ ìƒí’ˆ: {len(product_db):,}ê°œ")
        st.write(f"â€¢ ì¸ë±ìŠ¤: {'âœ…' if index else 'âŒ'}")
        st.write(f"â€¢ í˜„ì¬ ëª¨ë“œ: **{'ê¸°ì¡´íšŒì›' if user_mode == 'member' else 'ë¹„íšŒì›'}**")

    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        clear_memory()
        st.session_state.session = []
        st.rerun()

# -----------------------------------------
# ğŸ’¬ 10. Chat Interface
# -----------------------------------------
if "session" not in st.session_state:
    st.session_state.session = []

# í˜„ì¬ ëª¨ë“œ í‘œì‹œ
if user_mode == "member" and cid_input:
    st.caption(f"ğŸ¦ **ê¸°ì¡´ íšŒì› ëª¨ë“œ** | ê³ ê° ID: `{cid_input}` | ê°œì¸í™” ì¶”ì²œ í™œì„±í™”")
else:
    st.caption(f"ğŸ‘¤ **ë¹„íšŒì› ëª¨ë“œ** | í˜ë¥´ì†Œë‚˜: `{selected_persona}` | í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¶”ì²œ")

for role, msg in st.session_state.session:
    with st.chat_message(role):
        st.write(msg)

if user_msg := st.chat_input("ê¸ˆìœµ ìƒí’ˆì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”..."):
    st.session_state.session.append(("user", user_msg))
    with st.chat_message("user"):
        st.write(user_msg)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ë¶„ì„ ì¤‘..."):
            # ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì „ë‹¬
            reply = run_agent(
                user_input=user_msg,
                user_mode=user_mode,
                cid=cid_input if user_mode == "member" else None,
                persona=selected_persona if user_mode == "guest" else None
            )
            st.write(reply)
    st.session_state.session.append(("assistant", reply))

# -----------------------------------------
# ğŸ“Š 11. Dashboard (ê¸°ì¡´ íšŒì› ì „ìš©)
# -----------------------------------------
if user_mode == "member" and cid_input and 'customers_train' in data and not data['customers_train'].empty:
    user_vec = data['customers_train'][data['customers_train']['customer_id'] == cid_input]

    if not user_vec.empty:
        st.divider()
        st.subheader(f"ğŸ“Š FirstFin Insight: {cid_input}")

        col1, col2 = st.columns(2)

        target_cols = ['TOM_Invest', 'TOM_YOLO', 'TOM_Weekend', 'TOM_Digital', 'TOM_Cafe']
        valid_cols = [c for c in target_cols if c in user_vec.columns]

        if valid_cols:
            tom_metrics = user_vec[valid_cols].T
            tom_metrics.columns = ['Score']
            name_map = {'TOM_Invest': 'íˆ¬ìì„±í–¥', 'TOM_YOLO': 'YOLOì§€ìˆ˜', 'TOM_Weekend': 'ì£¼ë§ì†Œë¹„', 'TOM_Digital': 'ë””ì§€í„¸ì¹œí™”',
                        'TOM_Cafe': 'ì·¨í–¥(ì¹´í˜)'}
            tom_metrics.index = [name_map.get(c, c) for c in valid_cols]
            with col1:
                st.markdown("**ğŸ•µï¸â€â™‚ï¸ TOM ë¼ì´í”„ìŠ¤íƒ€ì¼ í”„ë¡œí•„**")
                st.bar_chart(tom_metrics, color="#4A90E2")

        with col2:
            st.markdown("**ğŸ“ˆ ì†Œë¹„ ì¦ê° ì¶”ì„¸**")
            trend_col = 'TOM_Trend_Raw' if 'TOM_Trend_Raw' in user_vec.columns else 'TOM_Trend'
            if trend_col in user_vec.columns:
                trend_val = user_vec[trend_col].values[0]
                base_point = 100
                df_trend = pd.DataFrame([base_point * (1 - trend_val), base_point, base_point * (1 + trend_val)],
                                        columns=['ì˜ˆìƒ ì†Œë¹„ íë¦„'], index=['ì§€ë‚œë‹¬', 'ì´ë²ˆë‹¬', 'ë‹¤ìŒë‹¬(ì˜ˆì¸¡)'])
                if trend_val > 0.05:
                    st.warning(f"ğŸš¨ ì†Œë¹„ ê¸‰ì¦! (+{trend_val:.1%})")
                    st.line_chart(df_trend, color="#FF4B4B")
                elif trend_val < -0.05:
                    st.success(f"âœ… ì ˆì•½ ëª¨ë“œ ({trend_val:.1%})")
                    st.line_chart(df_trend, color="#2ECC71")
                else:
                    st.info(f"âš–ï¸ ì•ˆì •ì  ({trend_val:.1%})")
                    st.line_chart(df_trend, color="#808495")